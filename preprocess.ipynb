{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import zarr\n",
    "from tqdm import tqdm\n",
    "import fastparquet\n",
    "from joblib import Parallel, delayed\n",
    "import cv2\n",
    "# specify the paths to high and low quality datasets\n",
    "hi_res_path = './datasets/High_Res_T2'\n",
    "low_res_path = './datasets/Low_Res_T2'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Data From nii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 22/578 [00:04<01:47,  5.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(slice_width):\n\u001b[0;32m     24\u001b[0m                 row \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame([[pt, i, file_name, \u001b[39m'\u001b[39m\u001b[39mT2\u001b[39m\u001b[39m'\u001b[39m, slice_width, voxel_size]], columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mslice_num\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfile_name\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMR_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mslice_width\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvoxel_size\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 25\u001b[0m                 df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat([df, row], ignore_index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     27\u001b[0m df\u001b[39m.\u001b[39mto_pickle(\u001b[39m'\u001b[39m\u001b[39mall_data.pkl\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dartmouth_guest\\.conda\\envs\\gabriel_ssl\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Dartmouth_guest\\.conda\\envs\\gabriel_ssl\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:381\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39mConcatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39m1   3   4\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    368\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    369\u001b[0m     objs,\n\u001b[0;32m    370\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    378\u001b[0m     sort\u001b[39m=\u001b[39msort,\n\u001b[0;32m    379\u001b[0m )\n\u001b[1;32m--> 381\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result()\n",
      "File \u001b[1;32mc:\\Users\\Dartmouth_guest\\.conda\\envs\\gabriel_ssl\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:616\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    612\u001b[0m             indexers[ax] \u001b[39m=\u001b[39m obj_labels\u001b[39m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    614\u001b[0m     mgrs_indexers\u001b[39m.\u001b[39mappend((obj\u001b[39m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 616\u001b[0m new_data \u001b[39m=\u001b[39m concatenate_managers(\n\u001b[0;32m    617\u001b[0m     mgrs_indexers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_axes, concat_axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbm_axis, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[0;32m    618\u001b[0m )\n\u001b[0;32m    619\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy:\n\u001b[0;32m    620\u001b[0m     new_data\u001b[39m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\Dartmouth_guest\\.conda\\envs\\gabriel_ssl\\lib\\site-packages\\pandas\\core\\internals\\concat.py:233\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    231\u001b[0m     fastpath \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m values\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    232\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 233\u001b[0m     values \u001b[39m=\u001b[39m _concatenate_join_units(join_units, concat_axis, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    234\u001b[0m     fastpath \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[39mif\u001b[39;00m fastpath:\n",
      "File \u001b[1;32mc:\\Users\\Dartmouth_guest\\.conda\\envs\\gabriel_ssl\\lib\\site-packages\\pandas\\core\\internals\\concat.py:577\u001b[0m, in \u001b[0;36m_concatenate_join_units\u001b[1;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[0;32m    574\u001b[0m     concat_values \u001b[39m=\u001b[39m ensure_block_shape(concat_values, \u001b[39m2\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m     concat_values \u001b[39m=\u001b[39m concat_compat(to_concat, axis\u001b[39m=\u001b[39;49mconcat_axis)\n\u001b[0;32m    579\u001b[0m \u001b[39mreturn\u001b[39;00m concat_values\n",
      "File \u001b[1;32mc:\\Users\\Dartmouth_guest\\.conda\\envs\\gabriel_ssl\\lib\\site-packages\\pandas\\core\\dtypes\\concat.py:151\u001b[0m, in \u001b[0;36mconcat_compat\u001b[1;34m(to_concat, axis, ea_compat_axis)\u001b[0m\n\u001b[0;32m    148\u001b[0m             to_concat \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m to_concat]\n\u001b[0;32m    149\u001b[0m             kinds \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mo\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m--> 151\u001b[0m result \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(to_concat, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m    152\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kinds \u001b[39mand\u001b[39;00m result\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    153\u001b[0m     \u001b[39m# GH#39817\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBehavior when concatenating bool-dtype and numeric-dtype arrays is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdeprecated; in a future version these will cast to object dtype \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    161\u001b[0m     )\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#create a dataframe to keep track of the data\n",
    "df = pd.DataFrame(columns=['pt', 'slice_num', 'file_name', 'MR_type', 'slice_width', 'voxel_size'])\n",
    "# use the `load` function from `nibabel` to load the image\n",
    "for root, dirs, files in os.walk(hi_res_path):\n",
    "    for fp in tqdm(files):\n",
    "        if '.gz' in fp:\n",
    "            pt = fp.split('-')[0]\n",
    "            file_name = fp\n",
    "\n",
    "            hi_res = nib.load(os.path.join(hi_res_path, fp))\n",
    "            low_res = nib.load(os.path.join(low_res_path, fp))\n",
    "\n",
    "            # get the image data\n",
    "            # hi_res_data = hi_res.get_fdata()\n",
    "            # low_res_data = low_res.get_fdata()\n",
    "\n",
    "            # get the image header\n",
    "            hi_res_header = hi_res.header\n",
    "            low_res_header = low_res.header\n",
    "            voxel_size = hi_res_header.get_zooms()\n",
    "            slice_width = hi_res_header.get_data_shape()[2]\n",
    "\n",
    "            for i in range(slice_width):\n",
    "                row = pd.DataFrame([[pt, i, file_name, 'T2', slice_width, voxel_size]], columns=['pt', 'slice_num', 'file_name', 'MR_type', 'slice_width', 'voxel_size'])\n",
    "                df = pd.concat([df, row], ignore_index=True)\n",
    "                \n",
    "df.to_pickle('all_data.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Into Train/Val/Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_pickle(\u001b[39m'\u001b[39m\u001b[39mall_data.pkl\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m pt_list \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()\n\u001b[0;32m      4\u001b[0m \u001b[39m# randomly select 80% of the patients for training\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('all_data.pkl')\n",
    "pt_list = df['pt'].unique()\n",
    "\n",
    "# randomly select 80% of the patients for training\n",
    "train_pt_list = np.random.choice(pt_list, int(len(pt_list)*0.8), replace=False)\n",
    "\n",
    "# the remaining patients are for testing\n",
    "test_pt_list = np.setdiff1d(pt_list, train_pt_list)\n",
    "\n",
    "#randomly select 20% of training patients for validation\n",
    "val_pt_list = np.random.choice(train_pt_list, int(len(train_pt_list)*0.15), replace=False)\n",
    "\n",
    "#remove val patients from train patients\n",
    "train_pt_list = np.setdiff1d(train_pt_list, val_pt_list)\n",
    "\n",
    "# create a dataframe for each of the training, validation, and testing sets\n",
    "train_df = df[df['pt'].isin(train_pt_list)]\n",
    "val_df = df[df['pt'].isin(val_pt_list)]\n",
    "test_df = df[df['pt'].isin(test_pt_list)]\n",
    "\n",
    "# save the dataframes to parquet files\n",
    "os.makedirs('split_datasets', exist_ok=True)\n",
    "os.makedirs('split_datasets/train', exist_ok=True)\n",
    "os.makedirs('split_datasets/val', exist_ok=True)\n",
    "os.makedirs('split_datasets/test', exist_ok=True)\n",
    "train_df.to_pickle('split_datasets/train_df.pkl')\n",
    "val_df.to_pickle('split_datasets/val_df.pkl')\n",
    "test_df.to_pickle('split_datasets/test_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 578/578 [00:00<00:00, 577766.38it/s]\n",
      "100%|██████████| 578/578 [09:08<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#load images and save as zarr files\n",
    "train_df = pd.read_pickle('split_datasets/train_df.pkl')\n",
    "val_df = pd.read_pickle('split_datasets/val_df.pkl')\n",
    "test_df = pd.read_pickle('split_datasets/test_df.pkl')\n",
    "\n",
    "train_pts = train_df['pt'].unique()\n",
    "val_pts = val_df['pt'].unique()\n",
    "test_pts = test_df['pt'].unique()\n",
    "\n",
    "gz_files = []\n",
    "for root, dirs, files in os.walk(hi_res_path):\n",
    "        for fp in tqdm(files):\n",
    "            if '.gz' in fp:\n",
    "                gz_files.append(fp)\n",
    "\n",
    "#create a parralelized function to save the images as zarr files\n",
    "def save_zarr(fp, hi_res_path, low_res_path, train_pts, val_pts, test_pts, resize_factor=0.5):\n",
    "            pt = fp.split('-')[0]\n",
    "            \n",
    "\n",
    "            hi_res = nib.load(os.path.join(hi_res_path, fp))\n",
    "            low_res = nib.load(os.path.join(low_res_path, fp))\n",
    "\n",
    "            # get the image data\n",
    "            hi_res_data = hi_res.get_fdata()\n",
    "            low_res_data = low_res.get_fdata()\n",
    "            #save each frame as a zarr file\n",
    "            for i in range(hi_res_data.shape[2]):\n",
    "                hi_res_frame = hi_res_data[:,:,i]\n",
    "                low_res_frame = low_res_data[:,:,i]\n",
    "                #resize image\n",
    "                low_res_frame = cv2.resize(low_res_frame, (128,128))\n",
    "                                \n",
    "                if pt in train_pts:\n",
    "                    assert pt not in val_pts and pt not in test_pts\n",
    "                    zarr.save(os.path.join('split_datasets/train', f'hi_res_{pt}_{i}.zarr'), hi_res_frame)\n",
    "                    zarr.save(os.path.join('split_datasets/train', f'low_res_{pt}_{i}.zarr'), low_res_frame)\n",
    "                elif pt in val_pts:\n",
    "                    assert pt not in train_pts and pt not in test_pts\n",
    "                    zarr.save(os.path.join('split_datasets/val', f'hi_res_{pt}_{i}.zarr'), hi_res_frame)\n",
    "                    zarr.save(os.path.join('split_datasets/val', f'low_res_{pt}_{i}.zarr'), low_res_frame)\n",
    "                elif pt in test_pts:\n",
    "                    assert pt not in train_pts and pt not in val_pts\n",
    "                    zarr.save(os.path.join('split_datasets/test', f'hi_res_{pt}_{i}.zarr'), hi_res_frame)\n",
    "                    zarr.save(os.path.join('split_datasets/test', f'low_res_{pt}_{i}.zarr'), low_res_frame)\n",
    "                else:\n",
    "                    raise ValueError('Patient not in any dataset')\n",
    "\n",
    "#parralelize this function using joblib\n",
    "Parallel(n_jobs=8, prefer='threads')(delayed(save_zarr)(fp, hi_res_path, low_res_path, train_pts, val_pts, test_pts) for fp in tqdm(gz_files))          \n",
    "print('done')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gabriel_ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
